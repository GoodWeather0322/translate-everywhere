{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用edgetts模擬錄音輸入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import edge_tts\n",
    "from edge_tts import VoicesManager\n",
    "\n",
    "\n",
    "async def dynamic_voice_selection(gender=\"Male\", language=\"zh\"):\n",
    "    voice_manager = await VoicesManager.create()\n",
    "    voices = voice_manager.find(Gender=gender, Language=language)\n",
    "    voice = random.choice(voices)[\"Name\"]\n",
    "    voice = 'zh-TW-YunJheNeural'\n",
    "    return voice\n",
    "\n",
    "async def lang_voice_sellection(lang):\n",
    "    voice_dict = {\n",
    "        'zh' : 'zh-TW-HsiaoChenNeural',\n",
    "        'en' : 'en-US-AvaNeural', \n",
    "        'ja' : 'ja-JP-KeitaNeural', \n",
    "        'ko' : 'ko-KR-HyunsuNeural'\n",
    "    }\n",
    "    if lang not in voice_dict:\n",
    "        print('lang not support at this time')\n",
    "        return False\n",
    "    \n",
    "    return voice_dict[lang]\n",
    "    \n",
    "\n",
    "async def edgetts_generate(text, voice, output): \n",
    "    communicate = edge_tts.Communicate(text, voice)\n",
    "    await communicate.save(output)\n",
    "\n",
    "text = \"晚上繼續寫程式\"\n",
    "language = \"zh\"\n",
    "output = \"test.wav\"\n",
    "voice = await dynamic_voice_selection(language=language)\n",
    "\n",
    "await edgetts_generate(text, voice, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['af', 'sq', 'am', 'ar', 'az', 'bn', 'bs', 'bg', 'my', 'ca', 'zh', 'hr', 'cs', 'da', 'nl', 'en', 'et', 'fil', 'fi', 'fr', 'gl', 'ka', 'de', 'el', 'gu', 'he', 'hi', 'hu', 'is', 'id', 'ga', 'it', 'ja', 'jv', 'kn', 'kk', 'km', 'ko', 'lo', 'lv', 'lt', 'mk', 'ms', 'ml', 'mt', 'mr', 'mn', 'ne', 'nb', 'ps', 'fa', 'pl', 'pt', 'ro', 'ru', 'sr', 'si', 'sk', 'sl', 'so', 'es', 'su', 'sw', 'sv', 'ta', 'te', 'th', 'tr', 'uk', 'ur', 'uz', 'vi', 'cy', 'zu'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voice_manager = await VoicesManager.create()\n",
    "voices = voice_manager.find()\n",
    "lang_code = {}\n",
    "for voice in voices:\n",
    "    if voice['Language'] not in lang_code:\n",
    "        lang_code[voice['Language']] = 1\n",
    "\n",
    "lang_code.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 對音檔進行whisper translate\n",
    "\n",
    "- whisper can only do X -> X (transcribe) and X -> English (translate)\n",
    "- 所以whisper訓練只有其他語言翻譯英文，並沒有其他語言翻譯其他語言的功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/anaconda3/envs/translate-everywhere/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid model size 'large-v3', expected one of: tiny.en, tiny, base.en, base, small.en, small, medium.en, medium, large-v1, large-v2, large",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m model_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlarge-v3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Run on GPU with FP16\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# or run on GPU with INT8\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mWhisperModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mint8_float16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# or run on CPU with INT8\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# for segment in segments:\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#     print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/translate-everywhere/lib/python3.10/site-packages/faster_whisper/transcribe.py:122\u001b[0m, in \u001b[0;36mWhisperModel.__init__\u001b[0;34m(self, model_size_or_path, device, device_index, compute_type, cpu_threads, num_workers, download_root, local_files_only)\u001b[0m\n\u001b[1;32m    120\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m model_size_or_path\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 122\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_size_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m ctranslate2\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mWhisper(\n\u001b[1;32m    129\u001b[0m     model_path,\n\u001b[1;32m    130\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m     inter_threads\u001b[38;5;241m=\u001b[39mnum_workers,\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    137\u001b[0m tokenizer_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/translate-everywhere/lib/python3.10/site-packages/faster_whisper/utils.py:72\u001b[0m, in \u001b[0;36mdownload_model\u001b[0;34m(size_or_id, output_dir, local_files_only, cache_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m     repo_id \u001b[38;5;241m=\u001b[39m _MODELS\u001b[38;5;241m.\u001b[39mget(size_or_id)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m repo_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     73\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid model size \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, expected one of: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m             \u001b[38;5;241m%\u001b[39m (size_or_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(_MODELS\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m     75\u001b[0m         )\n\u001b[1;32m     77\u001b[0m allow_patterns \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocabulary.*\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     82\u001b[0m ]\n\u001b[1;32m     84\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_files_only\u001b[39m\u001b[38;5;124m\"\u001b[39m: local_files_only,\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_patterns\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_patterns,\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm_class\u001b[39m\u001b[38;5;124m\"\u001b[39m: disabled_tqdm,\n\u001b[1;32m     88\u001b[0m }\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid model size 'large-v3', expected one of: tiny.en, tiny, base.en, base, small.en, small, medium.en, medium, large-v1, large-v2, large"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "\n",
    "model_size = \"large-v3\"\n",
    "\n",
    "\n",
    "# Run on GPU with FP16\n",
    "# model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\n",
    "\n",
    "# or run on GPU with INT8\n",
    "model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n",
    "# or run on CPU with INT8\n",
    "# model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
    "\n",
    "segments, info = model.transcribe(\"test.mp3\", beam_size=5, task='translate')\n",
    "\n",
    "print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "\n",
    "for segment in segments:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### llm translate\n",
    "- llama3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "def llm_translate(source_language, target_language, source_sentence):\n",
    "\n",
    "    multilingual_prompt_dict = {\n",
    "        \"en\" : 'How is the weather today', \n",
    "        'ja' : '今日の天気はどうですか', \n",
    "        'zh' : '今天的天氣如何', \n",
    "        'ko' : '방법 날씨가 오늘', \n",
    "    }\n",
    "\n",
    "    if source_language not in multilingual_prompt_dict:\n",
    "        print('source_language not support at this time')\n",
    "        return False\n",
    "    if target_language not in multilingual_prompt_dict:\n",
    "        print('target_language not support at this time')\n",
    "        return False\n",
    "\n",
    "    chat_model = ChatOllama(\n",
    "        base_url='http://localhost:11434',\n",
    "        model='llama3',\n",
    "        temperature=0,\n",
    "    )\n",
    "    prompt_texts = [\n",
    "        \"\"\"You are a helpful translator and only output the result in json format.\\nEvery word should be carefully translated.\\nTranslate this from <{source_language}> to <{target_language}>\\n\"\"\",\n",
    "        \"\"\"<{source_language}>:{source_sentence_example}\\n\"\"\", \n",
    "        \"\"\"<{target_language}>:{target_sentence_example}\\n\"\"\", \n",
    "        \"\"\"<{source_language}>:{source_sentence}\\n\"\"\", \n",
    "    ]\n",
    "\n",
    "    prompt_templates = []\n",
    "    for i, text in enumerate(prompt_texts):\n",
    "        if i == 0:\n",
    "            prompt_templates.append(SystemMessagePromptTemplate.from_template(text))\n",
    "        else:\n",
    "            case_number = int((i + 1) / 2)\n",
    "            if i % 2 == 1:\n",
    "                prompt_templates.append(\n",
    "                    HumanMessagePromptTemplate.from_template(\n",
    "                        f\"{text}\"\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                prompt_templates.append(\n",
    "                    AIMessagePromptTemplate.from_template(\n",
    "                        f\"{text}\"\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    chat_template = ChatPromptTemplate.from_messages(prompt_templates)\n",
    "\n",
    "    prompt_messages = []\n",
    "\n",
    "    source_sentence_example = multilingual_prompt_dict['zh']\n",
    "    target_sentence_example = multilingual_prompt_dict['ja']\n",
    "    prompt_message = chat_template.format_prompt(\n",
    "        source_language=source_language, \n",
    "        target_language=target_language, \n",
    "        source_sentence_example=source_sentence_example, \n",
    "        target_sentence_example=target_sentence_example, \n",
    "        source_sentence=source_sentence\n",
    "    )\n",
    "\n",
    "    prompt_messages.append(prompt_message.to_messages())\n",
    "\n",
    "    print(prompt_messages)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    generation = chat_model.generate(prompt_messages)\n",
    "    end_time = time.perf_counter()\n",
    "\n",
    "    print(f'time spend: {end_time - start_time}')\n",
    "\n",
    "    single_generation = generation.generations[0]\n",
    "    print(single_generation[0].text)\n",
    "    print('='*20)\n",
    "\n",
    "    return single_generation[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[SystemMessage(content='You are a helpful translator and only output the result in json format.\\nEvery word should be carefully translated.\\nTranslate this from <zh> to <ja>\\n'), HumanMessage(content='<zh>:今天的天氣如何\\n'), AIMessage(content='<ja>:今日の天気はどうですか\\n'), HumanMessage(content='<zh>:今天午餐要吃甚麼\\n')]]\n",
      "time spend: 3.2050712070195004\n",
      "<ja>:今日の昼食は何を食べますか\n",
      "====================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'今日の昼食は何を食べますか'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_language = 'zh'\n",
    "target_language = 'ja'\n",
    "source_sentence = '今天午餐要吃甚麼'\n",
    "output = llm_translate(source_language, target_language, source_sentence)\n",
    "translate_text = output.split(':')[1]\n",
    "translate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice = await lang_voice_sellection(target_language)\n",
    "output_file = 'test_translate.wav'\n",
    "await edgetts_generate(translate_text, voice, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 試一下 openvoice的TTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### translate wav voice conversion to source voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/disk1/chris/uaicraft_workspace/translate-everywhere/OpenVoice\")\n",
    "import os\n",
    "import torch\n",
    "from openvoice import se_extractor\n",
    "from openvoice.api import ToneColorConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/anaconda3/envs/translate-everywhere/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint '../OpenVoice/checkpoints_v2/converter/checkpoint.pth'\n",
      "missing/unexpected keys: [] []\n"
     ]
    }
   ],
   "source": [
    "ckpt_converter = '../OpenVoice/checkpoints_v2/converter'\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tone_color_converter = ToneColorConverter(f'{ckpt_converter}/config.json', device=device)\n",
    "tone_color_converter.load_ckpt(f'{ckpt_converter}/checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimating duration from bitrate, this may be inaccurate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenVoice version: v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimating duration from bitrate, this may be inaccurate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenVoice version: v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/anaconda3/envs/translate-everywhere/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "source_wav = 'test_translate.wav'\n",
    "target_wav = 'test.wav'\n",
    "\n",
    "source_se, source_audio_name = se_extractor.get_se(source_wav, tone_color_converter, vad=False)\n",
    "target_se, target_audio_name = se_extractor.get_se(target_wav, tone_color_converter, vad=False)\n",
    "\n",
    "save_path = 'test_translate_conversion.wav'\n",
    "\n",
    "encode_message = \"@MyShell\"\n",
    "tone_color_converter.convert(\n",
    "    audio_src_path=source_wav, \n",
    "    src_se=source_se, \n",
    "    tgt_se=target_se, \n",
    "    output_path=save_path,\n",
    "    message=encode_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchvision is not available - cannot save figures\n"
     ]
    }
   ],
   "source": [
    "## pyanonotate audio segment\n",
    "\n",
    "# instantiate the model\n",
    "from pyannote.audio import Model\n",
    "from pyannote.audio.pipelines import VoiceActivityDetection\n",
    "model = Model.from_pretrained(\n",
    "        \"pyannote/segmentation-3.0\", \n",
    "        use_auth_token=\"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\")\n",
    "\n",
    "\n",
    "pipeline = VoiceActivityDetection(segmentation=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPER_PARAMETERS = {\n",
    "  # remove speech regions shorter than that many seconds.\n",
    "  \"min_duration_on\": 0.0,\n",
    "  # fill non-speech regions shorter than that many seconds.\n",
    "  \"min_duration_off\": 0.0\n",
    "}\n",
    "pipeline.instantiate(HYPER_PARAMETERS)\n",
    "vad = pipeline(\"test_translate_conversion.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Segment(0.132219, 2.14034)>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vad.get_timeline().support())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Segment' object has no attribute 'word'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m vad\u001b[38;5;241m.\u001b[39mget_timeline()\u001b[38;5;241m.\u001b[39msupport()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstart\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m timeline \u001b[38;5;129;01min\u001b[39;00m vad\u001b[38;5;241m.\u001b[39mget_timeline()\u001b[38;5;241m.\u001b[39msupport():\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(timeline\u001b[38;5;241m.\u001b[39mstart, timeline\u001b[38;5;241m.\u001b[39mend, \u001b[43mtimeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Segment' object has no attribute 'word'"
     ]
    }
   ],
   "source": [
    "vad.get_timeline().support()[0].start\n",
    "\n",
    "for timeline in vad.get_timeline().support():\n",
    "    print(timeline.start, timeline.end, timeline.word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translate-everywhere",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
